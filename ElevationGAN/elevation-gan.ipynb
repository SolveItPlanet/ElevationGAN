{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681c04e5",
   "metadata": {
    "papermill": {
     "duration": 0.008724,
     "end_time": "2022-11-06T18:10:31.063483",
     "exception": false,
     "start_time": "2022-11-06T18:10:31.054759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTANT STARTING NOTES\n",
    "\n",
    "**Code has only been testing on GPU**\n",
    "\n",
    "In the code immediately below: \n",
    "1. Choose how to train - in the code below choose 0, 1 or 2 (optimizing parameters, fully training the model or just gathering training data)\n",
    "2. Choose paths to training data (leave normal if not appropriate)\n",
    "3. Choose number of files to generate (leave normal if not appropriate)\n",
    "4. Choose number of samples per file (leave normal if not appropriate)\n",
    "5. Choose discriminator parameters (if optimizing parameters fill in multiple rows, otherwize the top row is used to fully train)\n",
    "6. Choose batch size (if using TPU should be 128 per core otherwise refine to fit memory)\n",
    "7. Choose number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671bfcdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:10:31.076958Z",
     "iopub.status.busy": "2022-11-06T18:10:31.075821Z",
     "iopub.status.idle": "2022-11-06T18:10:31.090173Z",
     "shell.execute_reply": "2022-11-06T18:10:31.089374Z"
    },
    "papermill": {
     "duration": 0.022746,
     "end_time": "2022-11-06T18:10:31.092201",
     "exception": false,
     "start_time": "2022-11-06T18:10:31.069455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_type = 0\n",
    "training_paths = [\"../input/elevationdataset/elevation_data0(num_samples_ 32).npy\"]\n",
    "number_of_files = 1\n",
    "number_of_samples = 1\n",
    "\n",
    "discriminator_params = [\n",
    "    [96,(8,8),(16,16),128,(8,8),(16,16),8,(32,32),(1,1)],\n",
    "    [96,(8,8),(16,16),128,(8,8),(16,16),8,(32,32),(2,2)],\n",
    "    [96,(8,8),(16,16),128,(8,8),(16,16),8,(32,32),(4,4)],\n",
    "    [96,(8,8),(16,16),128,(8,8),(16,16),8,(32,32),(8,8)],\n",
    "    [96,(8,8),(16,16),128,(8,8),(16,16),8,(32,32),(16,16)],\n",
    "    [96,(8,8),(16,16),128,(8,8),(16,16),8,(32,32),(32,32)]\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 300\n",
    "\n",
    "save_num = 0  #Only changes the output file path when gathering more training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bef636",
   "metadata": {
    "papermill": {
     "duration": 0.004798,
     "end_time": "2022-11-06T18:10:31.102152",
     "exception": false,
     "start_time": "2022-11-06T18:10:31.097354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Setup**\n",
    "* *Install dependancies*\n",
    "* *Import dependancies*\n",
    "* *Set environment variables*\n",
    "* *Set strategy*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b528e3",
   "metadata": {
    "papermill": {
     "duration": 0.004858,
     "end_time": "2022-11-06T18:10:31.112444",
     "exception": false,
     "start_time": "2022-11-06T18:10:31.107586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## *Install Dependancies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce909a9f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-06T18:10:31.124181Z",
     "iopub.status.busy": "2022-11-06T18:10:31.123398Z",
     "iopub.status.idle": "2022-11-06T18:11:28.534944Z",
     "shell.execute_reply": "2022-11-06T18:11:28.533521Z"
    },
    "papermill": {
     "duration": 57.420199,
     "end_time": "2022-11-06T18:11:28.537779",
     "exception": false,
     "start_time": "2022-11-06T18:10:31.117580",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (2.19.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio) (1.21.6)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.7/site-packages (from imageio) (9.1.1)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting elevation\r\n",
      "  Downloading elevation-1.1.3-py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.7/site-packages (from elevation) (1.4.4)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from elevation) (8.0.4)\r\n",
      "Requirement already satisfied: fasteners in /opt/conda/lib/python3.7/site-packages (from elevation) (0.17.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->elevation) (4.13.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->elevation) (3.8.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->elevation) (4.1.1)\r\n",
      "Installing collected packages: elevation\r\n",
      "Successfully installed elevation-1.1.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.4)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\r\n",
      "Collecting h5py~=3.1.0\r\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\r\n",
      "Collecting numpy~=1.19.2\r\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\r\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\r\n",
      "Collecting typing-extensions<3.11,>=3.7\r\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.6.0)\r\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (5.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.19.4)\r\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\r\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\r\n",
      "Collecting tensorboard<2.7,>=2.6.0\r\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\r\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.15.0)\r\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.8.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.2.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.7)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2022.9.24)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.12)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\r\n",
      "Installing collected packages: typing-extensions, numpy, h5py, tensorboard\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.1.1\r\n",
      "    Uninstalling typing_extensions-4.1.1:\r\n",
      "      Successfully uninstalled typing_extensions-4.1.1\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.21.6\r\n",
      "    Uninstalling numpy-1.21.6:\r\n",
      "      Successfully uninstalled numpy-1.21.6\r\n",
      "  Attempting uninstall: h5py\r\n",
      "    Found existing installation: h5py 3.7.0\r\n",
      "    Uninstalling h5py-3.7.0:\r\n",
      "      Successfully uninstalled h5py-3.7.0\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.10.1\r\n",
      "    Uninstalling tensorboard-2.10.1:\r\n",
      "      Successfully uninstalled tensorboard-2.10.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\r\n",
      "dask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\r\n",
      "beatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\r\n",
      "tfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "rich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\r\n",
      "pytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\r\n",
      "pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\r\n",
      "nnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\r\n",
      "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\r\n",
      "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\r\n",
      "flax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "flake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\r\n",
      "featuretools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\r\n",
      "dask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\r\n",
      "dask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\r\n",
      "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\r\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\r\n",
      "allennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\r\n",
      "allennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\r\n",
      "aioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\r\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 typing-extensions-3.10.0.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.5.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (9.1.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.4.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.19.5)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (4.33.3)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (3.10.0.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install imageio\n",
    "#!pip install git+https://github.com/tensorflow/docs\n",
    "!pip install elevation\n",
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "!pip install PIL\n",
    "!pip install shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d7f9a",
   "metadata": {
    "papermill": {
     "duration": 0.015579,
     "end_time": "2022-11-06T18:11:28.570389",
     "exception": false,
     "start_time": "2022-11-06T18:11:28.554810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## *Import Dependancies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5348f1ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:28.612089Z",
     "iopub.status.busy": "2022-11-06T18:11:28.611608Z",
     "iopub.status.idle": "2022-11-06T18:11:33.260765Z",
     "shell.execute_reply": "2022-11-06T18:11:33.259557Z"
    },
    "papermill": {
     "duration": 4.669406,
     "end_time": "2022-11-06T18:11:33.262957",
     "exception": false,
     "start_time": "2022-11-06T18:11:28.593551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf             #For creating the networks\n",
    "import glob                         #Something to do with file (probably not necessary and should be removed)\n",
    "import imageio                      #Somethign to do with images (probably not necessary and should be removed)\n",
    "import matplotlib.pyplot as plt     #To plot elevation data\n",
    "import numpy as np                  #To deal with the arrays\n",
    "import os                           #To change system variables\n",
    "import PIL                          #To deal with images (specifically reading .tif files)\n",
    "from tensorflow.keras import layers #To create the network layers\n",
    "\n",
    "import shutil                       #To zip files (specifically the tensorboard logs)\n",
    "\n",
    "import datetime                     #Gets the current time and date (secifically to name the logs)\n",
    "\n",
    "import random                       #Creates random number and arrays to run the networks on\n",
    "import time                         #Gets the current time (specifically to measure time taken)\n",
    "import math                         #To run complex caluclation (specifically for calculating lat and long)\n",
    "\n",
    "from IPython import display         #To clear the output\n",
    "import elevation                    #To gather elevation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ea0ba",
   "metadata": {
    "papermill": {
     "duration": 0.008459,
     "end_time": "2022-11-06T18:11:33.280558",
     "exception": false,
     "start_time": "2022-11-06T18:11:33.272099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## *Set Environmental Variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c61b87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:33.299738Z",
     "iopub.status.busy": "2022-11-06T18:11:33.298500Z",
     "iopub.status.idle": "2022-11-06T18:11:33.304958Z",
     "shell.execute_reply": "2022-11-06T18:11:33.303550Z"
    },
    "papermill": {
     "duration": 0.017884,
     "end_time": "2022-11-06T18:11:33.306885",
     "exception": false,
     "start_time": "2022-11-06T18:11:33.289001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_malloc_async\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"  #Set GPU to be more memory efficient\n",
    "print(os.environ.get(\"TF_GPU_ALLOCATOR\"))             #Check system variable has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d23fad",
   "metadata": {
    "papermill": {
     "duration": 0.00853,
     "end_time": "2022-11-06T18:11:33.324172",
     "exception": false,
     "start_time": "2022-11-06T18:11:33.315642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## *Set Strategy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad018b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:33.343073Z",
     "iopub.status.busy": "2022-11-06T18:11:33.342305Z",
     "iopub.status.idle": "2022-11-06T18:11:33.346796Z",
     "shell.execute_reply": "2022-11-06T18:11:33.345796Z"
    },
    "papermill": {
     "duration": 0.01581,
     "end_time": "2022-11-06T18:11:33.348693",
     "exception": false,
     "start_time": "2022-11-06T18:11:33.332883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checks for TPU\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "except ValueError:\n",
    "    tpu = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd47740",
   "metadata": {
    "papermill": {
     "duration": 0.008472,
     "end_time": "2022-11-06T18:11:33.365768",
     "exception": false,
     "start_time": "2022-11-06T18:11:33.357296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup Necessary Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0946e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:33.384597Z",
     "iopub.status.busy": "2022-11-06T18:11:33.383804Z",
     "iopub.status.idle": "2022-11-06T18:11:36.186748Z",
     "shell.execute_reply": "2022-11-06T18:11:36.185679Z"
    },
    "papermill": {
     "duration": 2.814596,
     "end_time": "2022-11-06T18:11:36.188936",
     "exception": false,
     "start_time": "2022-11-06T18:11:33.374340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 18:11:33.456452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:33.552227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:33.553142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:33.556360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-06 18:11:33.556749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:33.557698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:33.558588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:35.813422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:35.814240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:35.814952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 18:11:35.815528: E tensorflow/core/common_runtime/gpu/gpu_process_state.cc:69] TF_GPU_ALLOCATOR=cuda_malloc_async environment found, but TensorFlow was not compiled with CUDA 11.2+.\n",
      "2022-11-06 18:11:35.815558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 60000 #Unsure what this does\n",
    "\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier) to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dfc41",
   "metadata": {
    "papermill": {
     "duration": 0.00859,
     "end_time": "2022-11-06T18:11:36.206688",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.198098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Collect Training Data\n",
    "* *Download elevation data*\n",
    "* *Read pre-downloaded elevation data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e61d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.226017Z",
     "iopub.status.busy": "2022-11-06T18:11:36.225718Z",
     "iopub.status.idle": "2022-11-06T18:11:36.236356Z",
     "shell.execute_reply": "2022-11-06T18:11:36.235366Z"
    },
    "papermill": {
     "duration": 0.022767,
     "end_time": "2022-11-06T18:11:36.238298",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.215531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(train_type == 2):\n",
    "    #Function to download new elevation\n",
    "    def get_elevation(num_samples):   #Num_sampels - number of elevation arrays to generate\n",
    "        global save_num  #Used to create file name\n",
    "        num_samples_start = num_samples #Used to record and print current elevation (so you can see where it is up to)\n",
    "        elevation_data = []  #Array to hold all elevation samples\n",
    "\n",
    "        while(num_samples > 0): #Loop to download correct number of samples\n",
    "            try:  #Try statement to stop faulty or missing elevation data (common occurance) from crashing entire program\n",
    "                #Top lat long cordinates\n",
    "                East1 = random.uniform(-180, 180)\n",
    "                North1 = random.uniform(-90, 90)\n",
    "\n",
    "                #Calculating lat coordinate 20km from previous\n",
    "                latitude_difference = 20/111.1945\n",
    "                North2 = North1 + latitude_difference\n",
    "\n",
    "                #Calculating long coordinate 20km from previous\n",
    "                longitute_difference = 20 / (111.1945 * math.cos(math.radians(North2)))\n",
    "                East2 = East1 + longitute_difference\n",
    "\n",
    "                #Gather elevation data as tif\n",
    "                elevation_tif = elevation.clip(bounds=(East1, North1, East2, North2), output=\"/content/tmp.tif\")\n",
    "\n",
    "                elevation_im = PIL.Image.open(\"/content/tmp.tif\") #Read .tif elevation file\n",
    "                elevation_im = elevation_im.resize((644,644))     #Resize elevation data to match others\n",
    "                elevation_np = np.array(elevation_im)             #Convert data to numpy array for easier storage and manipulation\n",
    "\n",
    "                elevation_data.append(elevation_np)  #Append to other samples\n",
    "\n",
    "                num_samples -= 1\n",
    "            except:\n",
    "                print(\"Exception\")\n",
    "\n",
    "        elevation_data = np.array(elevation_data) #Convert to numpy array\n",
    "        np.save(\"/content/elevation_files/elevation_data\" + str(save_num) + \"(num_samples: \" + str(num_samples_start) + \")\", elevation_data) #Save numpy array for later use\n",
    "        save_num += 1\n",
    "\n",
    "        #Proccess numpy array into tensor\n",
    "        elevation_data = elevation_data.reshape(elevation_data.shape[0], elevation_data.shape[1], elevation_data.shape[2], 1).astype('float32')\n",
    "        tf.convert_to_tensor(elevation_data)\n",
    "        elevation_dataset = tf.data.Dataset.from_tensor_slices(elevation_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "        return elevation_dataset\n",
    "    \n",
    "    for n in range(number_of_files):\n",
    "        get_elevation(number_of_samples)\n",
    "        shutil.make_archive(\"elevation_files\", \"zip\", \"elevation_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22917a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.257688Z",
     "iopub.status.busy": "2022-11-06T18:11:36.256911Z",
     "iopub.status.idle": "2022-11-06T18:11:36.263893Z",
     "shell.execute_reply": "2022-11-06T18:11:36.263025Z"
    },
    "papermill": {
     "duration": 0.018815,
     "end_time": "2022-11-06T18:11:36.265792",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.246977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(train_type < 2):\n",
    "    def load_elevation(paths):\n",
    "        elevation_data = np.load(paths[0])\n",
    "        first = True\n",
    "        for path in paths:\n",
    "            if(first != True):\n",
    "                elevation_data = np.append(elevation_data, np.load(path), 0)\n",
    "            first = False\n",
    "\n",
    "        elevation_data = elevation_data.reshape(elevation_data.shape[0], elevation_data.shape[1], elevation_data.shape[2], 1).astype('float32')\n",
    "\n",
    "        tf.convert_to_tensor(elevation_data)\n",
    "        elevation_dataset = tf.data.Dataset.from_tensor_slices(elevation_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "        \n",
    "        return elevation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d588d",
   "metadata": {
    "papermill": {
     "duration": 0.008542,
     "end_time": "2022-11-06T18:11:36.283282",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.274740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create GAN\n",
    "* *Define and create generator*\n",
    "* *Define and create discriminator*\n",
    "* *Define training loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9627564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.302451Z",
     "iopub.status.busy": "2022-11-06T18:11:36.301665Z",
     "iopub.status.idle": "2022-11-06T18:11:36.311142Z",
     "shell.execute_reply": "2022-11-06T18:11:36.310084Z"
    },
    "papermill": {
     "duration": 0.02103,
     "end_time": "2022-11-06T18:11:36.313083",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.292053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function to define generator network\n",
    "def make_generator_model():\n",
    "    #Input layer which will take a random array\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(23*23*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((23, 23, 256)))\n",
    "    assert model.output_shape == (None, 23, 23, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (3, 3), strides=(28, 28), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 644, 644, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 644, 644, 16)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (3, 3), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 644, 644, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc3d4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.332355Z",
     "iopub.status.busy": "2022-11-06T18:11:36.331533Z",
     "iopub.status.idle": "2022-11-06T18:11:36.339912Z",
     "shell.execute_reply": "2022-11-06T18:11:36.338904Z"
    },
    "papermill": {
     "duration": 0.020169,
     "end_time": "2022-11-06T18:11:36.341898",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.321729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"generator = make_generator_model()    #Create generator network\\n\\n\\n#Test the network with a random array\\nnoise = tf.random.normal([1, 100])  #Create random array\\ngenerated_image = generator(noise, training=False)   #Use generator with random array\\n\\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')  #Plot (show) generated image\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''generator = make_generator_model()    #Create generator network\n",
    "\n",
    "\n",
    "#Test the network with a random array\n",
    "noise = tf.random.normal([1, 100])  #Create random array\n",
    "generated_image = generator(noise, training=False)   #Use generator with random array\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')  #Plot (show) generated image'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4f447d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.361418Z",
     "iopub.status.busy": "2022-11-06T18:11:36.360652Z",
     "iopub.status.idle": "2022-11-06T18:11:36.368321Z",
     "shell.execute_reply": "2022-11-06T18:11:36.367472Z"
    },
    "papermill": {
     "duration": 0.019545,
     "end_time": "2022-11-06T18:11:36.370279",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.350734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function to define discriminator model (controlled with variables)\n",
    "def make_variable_discriminator_model(var1, var2, var3, var4, var5, var6, var7, var8, var9):\n",
    "    #Input layer takes real or fake elevation data\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(var1, var2, strides=var3, padding='same',\n",
    "                                     input_shape=[644, 644, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(var4, var5, strides=var6, padding='same',\n",
    "                                     input_shape=[644, 644, var1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(var7, var8, strides=var9, padding='same',\n",
    "                                     input_shape=[644, 644, var4]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    #Output layer has one neuron with output between 0 and 1 (using sigmoid activation)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation=tf.keras.activations.sigmoid))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8648024f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.389746Z",
     "iopub.status.busy": "2022-11-06T18:11:36.388861Z",
     "iopub.status.idle": "2022-11-06T18:11:36.394967Z",
     "shell.execute_reply": "2022-11-06T18:11:36.394026Z"
    },
    "papermill": {
     "duration": 0.017659,
     "end_time": "2022-11-06T18:11:36.396872",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.379213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Create discriminator\\ndiscriminator = make_discriminator_model()\\n\\n#Test the discriminator on previously generated image\\ndecision = discriminator(generated_image)\\nprint (decision)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Create discriminator\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "#Test the discriminator on previously generated image\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0766fd8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.416591Z",
     "iopub.status.busy": "2022-11-06T18:11:36.416090Z",
     "iopub.status.idle": "2022-11-06T18:11:36.464956Z",
     "shell.execute_reply": "2022-11-06T18:11:36.464086Z"
    },
    "papermill": {
     "duration": 0.060915,
     "end_time": "2022-11-06T18:11:36.466919",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.406004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return tf.nn.compute_average_loss(total_loss, global_batch_size=BATCH_SIZE)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return tf.nn.compute_average_loss(cross_entropy(tf.ones_like(fake_output), fake_output), global_batch_size=BATCH_SIZE)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adamax(1e-4)\n",
    "generator_optimizer = tf.keras.mixed_precision.LossScaleOptimizer(generator_optimizer)\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adamax(1e-4)\n",
    "discriminator_optimizer = tf.keras.mixed_precision.LossScaleOptimizer(discriminator_optimizer)\n",
    "\n",
    "'''checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = None'''\n",
    "\n",
    "# Define our metrics\n",
    "if(tpu == None):\n",
    "    train_disc_real_loss = tf.keras.metrics.Mean('train_disc_real_loss', dtype=tf.float32)\n",
    "    train_disc_real_accuracy = tf.keras.metrics.MeanSquaredError('train_disc_real_accuracy')\n",
    "\n",
    "    train_disc_fake_loss = tf.keras.metrics.Mean('train_disc_fake_loss', dtype=tf.float32)\n",
    "    train_disc_fake_accuracy = tf.keras.metrics.MeanSquaredError('train_disc_fake_accuracy')\n",
    "\n",
    "    train_gen_loss = tf.keras.metrics.Mean('train_gen_loss', dtype=tf.float32)\n",
    "    train_gen_accuracy = tf.keras.metrics.MeanSquaredError('train_gen_accuracy')\n",
    "\n",
    "run_num = 1\n",
    "\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(predictions[i, :, :, 0])\n",
    "    plt.axis('off')\n",
    "\n",
    "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n",
    "    \n",
    "Epoch_offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee038e5",
   "metadata": {
    "papermill": {
     "duration": 0.010559,
     "end_time": "2022-11-06T18:11:36.498040",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.487481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd0eac",
   "metadata": {
    "papermill": {
     "duration": 0.00973,
     "end_time": "2022-11-06T18:11:36.518097",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.508367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Single Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5afe6d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.540392Z",
     "iopub.status.busy": "2022-11-06T18:11:36.540019Z",
     "iopub.status.idle": "2022-11-06T18:11:36.561801Z",
     "shell.execute_reply": "2022-11-06T18:11:36.560943Z"
    },
    "papermill": {
     "duration": 0.036056,
     "end_time": "2022-11-06T18:11:36.563962",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.527906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(train_type == 1):\n",
    "    @tf.function\n",
    "    def train_step(images, generator_model, discriminator_model):\n",
    "        noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_images = generator_model(noise, training=True)\n",
    "\n",
    "            real_output = discriminator_model(images, training=True)\n",
    "            fake_output = discriminator_model(generated_images, training=True)\n",
    "\n",
    "            gen_loss = generator_loss(fake_output)\n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "            scaled_gen_loss = generator_optimizer.get_scaled_loss(gen_loss)\n",
    "            scaled_disc_loss = discriminator_optimizer.get_scaled_loss(disc_loss)\n",
    "\n",
    "            #Tensorboard Logs\n",
    "            if(tpu==None):\n",
    "                train_disc_real_loss(cross_entropy(tf.ones_like(real_output), real_output))\n",
    "                train_disc_real_accuracy(tf.ones_like(real_output), real_output)\n",
    "\n",
    "                train_disc_fake_loss(cross_entropy(tf.zeros_like(fake_output), fake_output))\n",
    "                train_disc_fake_accuracy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "                train_gen_loss(gen_loss)\n",
    "                train_gen_accuracy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "        scaled_gradients_of_generator = gen_tape.gradient(scaled_gen_loss, generator_model.trainable_variables)\n",
    "        scaled_gradients_of_discriminator = disc_tape.gradient(scaled_disc_loss, discriminator_model.trainable_variables)\n",
    "\n",
    "        gradients_of_generator = generator_optimizer.get_unscaled_gradients(scaled_gradients_of_generator)\n",
    "        gradients_of_discriminator = discriminator_optimizer.get_unscaled_gradients(scaled_gradients_of_discriminator)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_model.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_model.trainable_variables))\n",
    "    #####END FUNCTION#####\n",
    "\n",
    "    #####TRAIN FUNCTION#####\n",
    "    def train(dataset, epochs, zip_after, generator_model, discriminator_model):\n",
    "        if(tpu==None):\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "            test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "            train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "            test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            for image_batch in dataset:\n",
    "                train_step(image_batch, generator_model, discriminator_model)\n",
    "\n",
    "            if(tpu==None):\n",
    "                with train_summary_writer.as_default():\n",
    "                    tf.summary.scalar('train_disc_real_loss:', train_disc_real_loss.result(), step=epoch+Epoch_offset)\n",
    "                    tf.summary.scalar('train_disc_real_accuracy:', train_disc_real_accuracy.result(), step=epoch+Epoch_offset)\n",
    "\n",
    "                    tf.summary.scalar('train_disc_fake_loss:', train_disc_fake_loss.result(), step=epoch+Epoch_offset)\n",
    "                    tf.summary.scalar('train_disc_fake_accuracy:', train_disc_fake_accuracy.result(), step=epoch+Epoch_offset)\n",
    "\n",
    "                    tf.summary.scalar('train_gen_loss:', train_gen_loss.result(), step=epoch+Epoch_offset)\n",
    "                    tf.summary.scalar('train_gen_accuracy:', train_gen_accuracy.result(), step=epoch+Epoch_offset)\n",
    "\n",
    "            # Produce images for the GIF as you go\n",
    "            display.clear_output(wait=True)\n",
    "            generate_and_save_images(generator_model,\n",
    "                                     epoch + 1,\n",
    "                                     seed)\n",
    "\n",
    "            # Save the model every 15 epochs\n",
    "            '''if (epoch + 1) % 15 == 0:\n",
    "              checkpoint.save(file_prefix = checkpoint_prefix)'''\n",
    "\n",
    "            if ((epoch + 1) % zip_after and tpu == None) == 0:\n",
    "                shutil.make_archive(\"logs\", \"zip\", \"logs\")\n",
    "\n",
    "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "\n",
    "        #Reset tensorboard writers\n",
    "        if(tpu==None):\n",
    "            train_disc_real_loss.reset_states()\n",
    "            train_disc_real_accuracy.reset_states()\n",
    "\n",
    "            train_disc_fake_loss.reset_states()\n",
    "            train_disc_fake_accuracy.reset_states()\n",
    "\n",
    "            train_gen_loss.reset_states()\n",
    "            train_gen_accuracy.reset_states()\n",
    "\n",
    "      # Generate after the final epoch\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator_model,\n",
    "                                   epochs,\n",
    "                                   seed)\n",
    "    ####END FUNCTION####\n",
    "\n",
    "    gen_model = make_generator_model()    #Create generator network\n",
    "    disc_model = make_variable_discriminator_model(discriminator_params[0,0], discriminator_params[0,1], discriminator_params[0,2], discriminator_params[0,3], discriminator_params[0,4], discriminator_params[0,5], discriminator_params[0,6], discriminator_params[0,7], discriminator_params[0,8]) #Create Discriminator\n",
    "    \n",
    "    train(load_elevation(training_paths), EPOCHS, 5, gen_model, disc_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576f21a",
   "metadata": {
    "papermill": {
     "duration": 0.009012,
     "end_time": "2022-11-06T18:11:36.582504",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.573492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Multiple Trains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c215149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.604270Z",
     "iopub.status.busy": "2022-11-06T18:11:36.603526Z",
     "iopub.status.idle": "2022-11-06T18:11:36.624390Z",
     "shell.execute_reply": "2022-11-06T18:11:36.623541Z"
    },
    "papermill": {
     "duration": 0.0337,
     "end_time": "2022-11-06T18:11:36.626489",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.592789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(train_type == 0):\n",
    "    #Function to train the network on different parameters (easier to identify the best parameters)\n",
    "    def variable_train(gen_params, disc_params, training_data, num_epochs, zip_after):\n",
    "        train_num = 0\n",
    "        for gen_param in gen_params:\n",
    "            for disc_param in disc_params:\n",
    "                train_num += 1\n",
    "                gen_model = make_generator_model()    #Create generator network\n",
    "                disc_model = make_variable_discriminator_model(disc_param[0], disc_param[1], disc_param[2], disc_param[3], disc_param[4], disc_param[5], disc_param[6], disc_param[7], disc_param[8]) #Create Discriminator\n",
    "\n",
    "                '''checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                     discriminator_optimizer=discriminator_optimizer,\n",
    "                                     generator=generator,\n",
    "                                     discriminator=discriminator)'''\n",
    "\n",
    "                @tf.function\n",
    "                def train_step(images, generator_model, discriminator_model):\n",
    "                    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "                    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                        generated_images = generator_model(noise, training=True)\n",
    "\n",
    "                        real_output = discriminator_model(images, training=True)\n",
    "                        fake_output = discriminator_model(generated_images, training=True)\n",
    "\n",
    "                        gen_loss = generator_loss(fake_output)\n",
    "                        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "                        scaled_gen_loss = generator_optimizer.get_scaled_loss(gen_loss)\n",
    "                        scaled_disc_loss = discriminator_optimizer.get_scaled_loss(disc_loss)\n",
    "\n",
    "                        if(tpu==None):\n",
    "                            #Tensorboard Logs\n",
    "                            train_disc_real_loss(cross_entropy(tf.ones_like(real_output), real_output))\n",
    "                            train_disc_real_accuracy(tf.ones_like(real_output), real_output)\n",
    "\n",
    "                            train_disc_fake_loss(cross_entropy(tf.zeros_like(fake_output), fake_output))\n",
    "                            train_disc_fake_accuracy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "                            train_gen_loss(gen_loss)\n",
    "                            train_gen_accuracy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "                    scaled_gradients_of_generator = gen_tape.gradient(scaled_gen_loss, generator_model.trainable_variables)\n",
    "                    scaled_gradients_of_discriminator = disc_tape.gradient(scaled_disc_loss, discriminator_model.trainable_variables)\n",
    "\n",
    "                    gradients_of_generator = generator_optimizer.get_unscaled_gradients(scaled_gradients_of_generator)\n",
    "                    gradients_of_discriminator = discriminator_optimizer.get_unscaled_gradients(scaled_gradients_of_discriminator)\n",
    "\n",
    "                    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_model.trainable_variables))\n",
    "                    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_model.trainable_variables))\n",
    "                #####END FUNCTION#####\n",
    "\n",
    "                #####TRAIN FUNCTION#####\n",
    "                def train(dataset, epochs, zip_after, generator_model, discriminator_model):\n",
    "                    if(tpu==None):\n",
    "                        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                        train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "                        test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "                        train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "                        test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        start = time.time()\n",
    "\n",
    "                        for image_batch in dataset:\n",
    "                            train_step(image_batch, generator_model, discriminator_model)\n",
    "\n",
    "                        if(tpu==None):\n",
    "                            with train_summary_writer.as_default():\n",
    "                                tf.summary.scalar('train_disc_real_loss:', train_disc_real_loss.result(), step=epoch+Epoch_offset)\n",
    "                                tf.summary.scalar('train_disc_real_accuracy:', train_disc_real_accuracy.result(), step=epoch+Epoch_offset)\n",
    "\n",
    "                                tf.summary.scalar('train_disc_fake_loss:', train_disc_fake_loss.result(), step=epoch+Epoch_offset)\n",
    "                                tf.summary.scalar('train_disc_fake_accuracy:', train_disc_fake_accuracy.result(), step=epoch+Epoch_offset)\n",
    "\n",
    "                                tf.summary.scalar('train_gen_loss:', train_gen_loss.result(), step=epoch+Epoch_offset)\n",
    "                                tf.summary.scalar('train_gen_accuracy:', train_gen_accuracy.result(), step=epoch+Epoch_offset)\n",
    "\n",
    "                        # Produce images for the GIF as you go\n",
    "                        display.clear_output(wait=True)\n",
    "                        generate_and_save_images(generator_model,\n",
    "                                                 epoch + 1,\n",
    "                                                 seed)\n",
    "\n",
    "                        # Save the model every 15 epochs\n",
    "                        '''if (epoch + 1) % 15 == 0:\n",
    "                          checkpoint.save(file_prefix = checkpoint_prefix)'''\n",
    "\n",
    "                        if ((epoch + 1) % zip_after and tpu == None) == 0:\n",
    "                            shutil.make_archive(\"logs\", \"zip\", \"logs\")\n",
    "\n",
    "                        print ('Train number: {} - Time for epoch {} is {} sec'.format(train_num, epoch + 1, time.time()-start))\n",
    "\n",
    "\n",
    "                    #Reset tensorboard writers\n",
    "                    if(tpu==None):\n",
    "                        train_disc_real_loss.reset_states()\n",
    "                        train_disc_real_accuracy.reset_states()\n",
    "\n",
    "                        train_disc_fake_loss.reset_states()\n",
    "                        train_disc_fake_accuracy.reset_states()\n",
    "\n",
    "                        train_gen_loss.reset_states()\n",
    "                        train_gen_accuracy.reset_states()\n",
    "\n",
    "                  # Generate after the final epoch\n",
    "                    display.clear_output(wait=True)\n",
    "                    generate_and_save_images(generator_model,\n",
    "                                               epochs,\n",
    "                                               seed)\n",
    "                ####END FUNCTION####\n",
    "                train(training_data, num_epochs, zip_after, gen_model, disc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d00d2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-06T18:11:36.646448Z",
     "iopub.status.busy": "2022-11-06T18:11:36.646173Z",
     "iopub.status.idle": "2022-11-06T19:24:01.465008Z",
     "shell.execute_reply": "2022-11-06T19:24:01.463999Z"
    },
    "papermill": {
     "duration": 4344.8321,
     "end_time": "2022-11-06T19:24:01.468036",
     "exception": false,
     "start_time": "2022-11-06T18:11:36.635936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator_params = [0] #Temporary until I code this to work\n",
    "\n",
    "variable_train(generator_params, discriminator_params, load_elevation(training_paths), EPOCHS, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bb676",
   "metadata": {
    "papermill": {
     "duration": 0.009531,
     "end_time": "2022-11-06T19:24:01.487520",
     "exception": false,
     "start_time": "2022-11-06T19:24:01.477989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logging\n",
    "In order to plot the logs:\n",
    "\n",
    "**Windows**\n",
    "1. Download the zip file\n",
    "2. Extract the zip file\n",
    "3. Open command line\n",
    "4. Run command: tensorboard --logdir=logs --host=localhost    *(Where logs is the directory to the unzipped log file)*\n",
    "5. Open a browser and type in the address bar: localhost:6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4420.993871,
   "end_time": "2022-11-06T19:24:04.594083",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-06T18:10:23.600212",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
